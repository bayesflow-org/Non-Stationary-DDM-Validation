{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of annoying tf warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "# import bayesflow as beef\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from experiment import ModelComparisonExperiment\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import binom, median_abs_deviation\n",
    "from sklearn.metrics import confusion_matrix, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES= [\n",
    "    'Random walk DDM', 'Mixture random walk DDM',\n",
    "    'Levy flight DDM', 'Regime switching DDM'\n",
    "    ]\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 4\n",
    "ENSEMBLE_SIZE = 10\n",
    "SIMULATION_PER_MODEL = 10000\n",
    "CHUNCK_SIZE = 200\n",
    "MODEL_NAMES = [\n",
    "    'Random walk DDM', 'Mixture random walk DDM',\n",
    "    'Levy flight DDM', 'Regime switching DDM'\n",
    "    ]\n",
    "\n",
    "with open('../data/validation_data.pkl', 'rb') as f:\n",
    "    validation_data = pickle.load(f)\n",
    "\n",
    "configurator = beef.configuration.DefaultModelComparisonConfigurator(NUM_MODELS)\n",
    "\n",
    "model_indices = tf.one_hot(np.tile(np.repeat(\n",
    "    [0, 1, 2, 3], CHUNCK_SIZE), int((SIMULATION_PER_MODEL/CHUNCK_SIZE))), NUM_MODELS\n",
    "    )\n",
    "\n",
    "def get_model_probablities(trainer):\n",
    "    model_probs = np.zeros((int(SIMULATION_PER_MODEL*NUM_MODELS), NUM_MODELS))\n",
    "    chunks = np.arange(0, SIMULATION_PER_MODEL+1, CHUNCK_SIZE)\n",
    "    for i in range(len(chunks)-1):\n",
    "        sim_1 = {'sim_data': validation_data['model_outputs'][0]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "        sim_2 = {'sim_data': validation_data['model_outputs'][1]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "        sim_3 = {'sim_data': validation_data['model_outputs'][2]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "        sim_4 = {'sim_data': validation_data['model_outputs'][3]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "\n",
    "        tmp_validation_data = {\n",
    "            'model_outputs': [sim_1, sim_2, sim_3, sim_4],\n",
    "            'model_indices': validation_data['model_indices']\n",
    "        }\n",
    "\n",
    "        tmp_validation_data_configured = configurator(tmp_validation_data)\n",
    "        with tf.device('/cpu:0'):\n",
    "            model_probs[(chunks[i]*NUM_MODELS):(chunks[i+1]*NUM_MODELS)] = trainer.amortizer.posterior_probs(\n",
    "                tmp_validation_data_configured\n",
    "            )\n",
    "    return model_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "trainer = ModelComparisonExperiment(\n",
    "        checkpoint_path=f'../checkpoints/ensemble_{i}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probs_per_ensemble = np.zeros((ENSEMBLE_SIZE, SIMULATION_PER_MODEL*NUM_MODELS, NUM_MODELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probs_per_ensemble[0] = get_model_probablities(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probs = np.zeros((int(SIMULATION_PER_MODEL*NUM_MODELS), NUM_MODELS))\n",
    "chunks = np.arange(SIMULATION_PER_MODEL+1, CHUNCK_SIZE)\n",
    "chunks\n",
    "# for i in range(len(chunks)-1):\n",
    "#     sim_1 = {'sim_data': validation_data['model_outputs'][0]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "#     sim_2 = {'sim_data': validation_data['model_outputs'][1]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "#     sim_3 = {'sim_data': validation_data['model_outputs'][2]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "#     sim_4 = {'sim_data': validation_data['model_outputs'][3]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "\n",
    "#     tmp_validation_data = {\n",
    "#         'model_outputs': [sim_1, sim_2, sim_3, sim_4],\n",
    "#         'model_indices': validation_data['model_indices']\n",
    "#     }\n",
    "\n",
    "#     tmp_validation_data_configured = configurator(tmp_validation_data)\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         model_probs[(chunks[i]*NUM_MODELS):(chunks[i+1]*NUM_MODELS)] = trainer.amortizer.posterior_probs(\n",
    "#             tmp_validation_data_configured\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ENSEMBLE_SIZE):\n",
    "    # initialize trainer\n",
    "    experiment = ModelComparisonExperiment(\n",
    "        checkpoint_path=f'checkpoints/ensemble_{i}'\n",
    "        )\n",
    "    # read training and validation data\n",
    "    with open('../data/training_data.pkl', 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    with open('../data/training_validation_data.pkl', 'rb') as f:\n",
    "        training_validation_data = pickle.load(f)\n",
    "    # start training\n",
    "    history = experiment.run(\n",
    "        training_data=training_data,\n",
    "        validation_data=training_validation_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/validation_data.pkl', 'rb') as f:\n",
    "    validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['model_outputs'][3]['sim_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurator = beef.configuration.DefaultModelComparisonConfigurator(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probs = np.zeros((40000, 4))\n",
    "chunks = np.arange(0, 10001, 200)\n",
    "for i in range(len(chunks)-1):\n",
    "    sim_1 = {'sim_data': validation_data['model_outputs'][0]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "    sim_2 = {'sim_data': validation_data['model_outputs'][1]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "    sim_3 = {'sim_data': validation_data['model_outputs'][2]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "    sim_4 = {'sim_data': validation_data['model_outputs'][3]['sim_data'][chunks[i]:chunks[i+1]]}\n",
    "    \n",
    "    tmp_validation_data = {\n",
    "        'model_outputs': [sim_1, sim_2, sim_3, sim_4],\n",
    "        'model_indices': validation_data['model_indices']\n",
    "    }\n",
    "    \n",
    "    tmp_validation_data_configured = configurator(tmp_validation_data)\n",
    "    with tf.device('/cpu:0'):\n",
    "        model_probs[(chunks[i]*4):(chunks[i+1]*4)] = experiment.amortizer.posterior_probs(\n",
    "            tmp_validation_data_configured\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indices = tf.one_hot(np.tile(np.repeat([0, 1, 2, 3], 200), 50), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_curves = beef.diagnostics.plot_calibration_curves(\n",
    "    true_models=model_indices,\n",
    "    pred_models=model_probs,\n",
    "    model_names=['Random walk',\n",
    "                 'Mixture random walk',\n",
    "                 'Levy flight',\n",
    "                 'Regime switching'],\n",
    "    fig_size=(14, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_indices = np.load(\"model_indices.npy\")\n",
    "model_probs = np.load(\"model_probs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_curves = beef.diagnostics.plot_calibration_curves(\n",
    "    true_models=model_indices,\n",
    "    pred_models=model_probs,\n",
    "    model_names=MODEL_NAMES,\n",
    "    fig_size=(18, 4),\n",
    "    title_fontsize=22,\n",
    "    label_fontsize=20,\n",
    "    tick_fontsize=18,\n",
    "    legend_fontsize=18\n",
    "    )\n",
    "\n",
    "cal_curves.savefig(\"../plots/calibration_curves.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    true_models,\n",
    "    pred_models,\n",
    "    model_names=None,\n",
    "    fig_size=(5, 5),\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=14,\n",
    "    value_fontsize=1,\n",
    "    xtick_rotation=None,\n",
    "    ytick_rotation=None,\n",
    "    normalize=True,\n",
    "    cmap=None,\n",
    "    title=True,\n",
    "):\n",
    "    \"\"\"Plots a confusion matrix for validating a neural network trained for Bayesian model comparison.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_models    : np.ndarray of shape (num_data_sets, num_models)\n",
    "        The one-hot-encoded true model indices per data set.\n",
    "    pred_models    : np.ndarray of shape (num_data_sets, num_models)\n",
    "        The predicted posterior model probabilities (PMPs) per data set.\n",
    "    model_names    : list or None, optional, default: None\n",
    "        The model names for nice plot titles. Inferred if None.\n",
    "    fig_size       : tuple or None, optional, default: (5, 5)\n",
    "        The figure size passed to the ``matplotlib`` constructor. Inferred if ``None``\n",
    "    title_fontsize : int, optional, default: 18\n",
    "        The font size of the title text.\n",
    "    label_fontsize    : int, optional, default: 16\n",
    "        The font size of the y-label and y-label texts\n",
    "    tick_fontsize  : int, optional, default: 12\n",
    "        The font size of the axis label and model name texts.\n",
    "    xtick_rotation: int, optional, default: None\n",
    "        Rotation of x-axis tick labels (helps with long model names).\n",
    "    ytick_rotation: int, optional, default: None\n",
    "        Rotation of y-axis tick labels (helps with long model names).\n",
    "    normalize      : bool, optional, default: True\n",
    "        A flag for normalization of the confusion matrix.\n",
    "        If True, each row of the confusion matrix is normalized to sum to 1.\n",
    "    cmap           : matplotlib.colors.Colormap or str, optional, default: None\n",
    "        Colormap to be used for the cells. If a str, it should be the name of a registered colormap,\n",
    "        e.g., 'viridis'. Default colormap matches the BayesFlow defaults by ranging from white to red.\n",
    "    title          : bool, optional, default True\n",
    "        A flag for adding 'Confusion Matrix' above the matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig : plt.Figure - the figure instance for optional saving\n",
    "    \"\"\"\n",
    "\n",
    "    if model_names is None:\n",
    "        num_models = true_models.shape[-1]\n",
    "        model_names = [rf\"$M_{{{m}}}$\" for m in range(1, num_models + 1)]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = LinearSegmentedColormap.from_list(\"\", [\"white\", \"#8f2727\"])\n",
    "\n",
    "    # Flatten input\n",
    "    true_models = np.argmax(true_models, axis=1)\n",
    "    pred_models = np.argmax(pred_models, axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_models, pred_models)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Initialize figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax, shrink=0.7)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]))\n",
    "    ax.set_xticklabels(model_names, fontsize=tick_fontsize)\n",
    "    if xtick_rotation:\n",
    "        plt.xticks(rotation=xtick_rotation, ha=\"right\")\n",
    "    ax.set_yticklabels(model_names, fontsize=tick_fontsize)\n",
    "    if ytick_rotation:\n",
    "        plt.yticks(rotation=ytick_rotation)\n",
    "    ax.set_xlabel(\"Predicted model\", fontsize=label_fontsize)\n",
    "    ax.set_ylabel(\"True model\", fontsize=label_fontsize)\n",
    "    # sns.set(font_scale=value_fontsize)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "    if title:\n",
    "        ax.set_title(\"Confusion Matrix\", fontsize=title_fontsize)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_confusion_matrix(\n",
    "    model_indices,\n",
    "    model_probs,\n",
    "    xtick_rotation=45,\n",
    "    ytick_rotation=0,\n",
    "    model_names=MODEL_NAMES,\n",
    "    fig_size=(8, 8),\n",
    "    title_fontsize=24,\n",
    "    label_fontsize=22,\n",
    "    value_fontsize=2,\n",
    "    tick_fontsize=18\n",
    ")\n",
    "\n",
    "# confusion_matrix.savefig(\"../plots/confusion_matrix.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs(model_indices - model_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(model_indices, model_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
